---
title: "Sentiment computation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.width = 7, fig.height = 4, fig.align = "center")
```

This tutorial provides a guide on how to perform the textual sentiment computation.

#### Preparation

```{r}
library("sentometrics")
library("quanteda")
library("stringi")
library("lexicon")
library("tm")

data("usnews")
data("list_lexicons")
data("list_valence_shifters")
```

### Ready-to-use sentiment calculation

A simple calculation of sentiment. The score is a substraction of the number of negative lexicon words (those with a score of --1) from the number of positive lexicon words (those with a score of 1).

```{r}
s <- compute_sentiment(
  usnews[["texts"]],
  sento_lexicons(list_lexicons[c("GI_en", "LM_en")]),
  how = "counts"
)
s
```

### Sentiment calculation from a `sento_corpus` object 

The same simple calculation as in Example 1, but using a `sento_corpus` object and the metadata features in the corpus.

```{r}
corpus <- sento_corpus(usnews)
corpus
```

```{r}
lexicons <- sento_lexicons(list_lexicons[c("GI_en", "LM_en", "HENRY_en")])

s <- compute_sentiment(corpus, lexicons, how = "counts")
head(s)
```

### Sentiment calculation from a **`tm`** `SimpleCorpus` object

Again, a simple textual sentiment calculation, but this time using a **`tm`** package corpus object. Super flexible! The output is this time slightly different, as the scores are divided by the total number of words.

```{r}
corpus <- SimpleCorpus(VectorSource(usnews[["texts"]]))
lexicons <- sento_lexicons(list_lexicons[c("GI_en", "LM_en", "HENRY_en")])

s <- compute_sentiment(corpus, lexicons, how = "proportional")
s
```

### Sentiment calculation from own tokenization

Even more flexibility in this example! You tokenize your corpus outside the sentiment computation function call, so you control exactly which words the lexicons are going to look into.

```{r}
corpus <- sento_corpus(usnews)
lexicons <- sento_lexicons(list_lexicons[c("GI_en", "LM_en", "HENRY_en")])

tks <- as.list(tokens(corpus, what = "fastestword"))

s <- compute_sentiment(corpus, lexicons, how = "counts", tokens = tks)
```

### Sentence-level sentiment calculation and aggregation

A textual sentiment computation on sentence-level, starting from a document-level corpus, and normalized dividing by the number of detected polarized words. Subsequently, the resulting sentence-level scores are aggregated into document-level scores.

```{r}
corpus <- sento_corpus(usnews[, 1:3])

s <- compute_sentiment(
  corpus,
  sento_lexicons(list_lexicons["LM_en"]),
  how = "proportionalPol",
  do.sentence = TRUE
)
s
```

```{r}
sDocs <- aggregate(s, ctr_agg(howDocs = "proportional"), do.full = FALSE)
sDocs
```

From these sentiment scores, we find the 4 documents where most positive sentiment scores were detected.

```{r}
peakDocsPos <- peakdocs(sDocs, n = 4, type = "pos")
peakDocsPos
```

```{r}
corpusPeaks <- corpus_subset(corpus, docnames(corpus) %in% peakDocsPos)
```

### Tf-idf sentiment calculation as in the **`quanteda`** package

The term frequency-inverse document frequency statistic is widely used to quantify term importance in a corpus. Its use extends to sentiment calculation simply by adding the polarity of the words to the equation. This example shows that the tf-idf sentiment output from **`sentometrics`** is the same as the output obtained using the text mining package **`quanteda`**.

```{r}
# ensure same tokenization for full comparability
txts <- usnews$texts[1:100]
toks <- stri_split_boundaries(stri_trans_tolower(txts), type = "word", 
                              skip_word_none = TRUE)

# pick a lexicon
lexIn <- list_lexicons$GI_en

# quanteda tf-idf sentiment calculation
dfmQ <- dfm(as.tokens(toks)) %>% dfm_tfidf(k = 1)

posWords <- lexIn[y == 1, x]
negWords <- lexIn[y == -1, x]

posScores <- rowSums(dfm_select(dfmQ, posWords))
negScores <- rowSums(dfm_select(dfmQ, negWords))
q <- unname(posScores - negScores)

# sentometrics tf-idf sentiment calculation
lex <- sento_lexicons(list(L = lexIn))
s <- compute_sentiment(txts, lex, tokens = toks, "TFIDF")[["L"]]
```

_R_ they equal?

```{r}
all.equal(q, s)
```

### The three key approaches to the sentiment computation

We offer three main approaches to do the lexicon-based sentiment calculation: account only for unigrams _(simple)_, consider valence shifting in a bigrams perspective _(valence)_, or consider valence shifting in a cluster of words around a detected polarized word _(cluster)_. Read the [vignette](https://ssrn.com/abstract=3067734) for more details! Here we demonstrate how to plot the different approaches for comparison.

```{r}
txts <- usnews[1:200, "texts"]

vals <- list_valence_shifters[["en"]]

lexValence <- sento_lexicons(list(nrc = hash_sentiment_nrc), vals[, c("x", "y")])
lexCluster <- sento_lexicons(list(nrc = hash_sentiment_nrc), vals[, c("x", "t")])

s1 <- compute_sentiment(txts, head(lexValence, -1))$nrc
s2 <- compute_sentiment(txts, lexValence)$nrc
s3 <- compute_sentiment(txts, lexCluster)$nrc
s <- cbind(simple = s1, valence = s2, cluster = s3)

matplot(s, type = "l", lty = 1, ylab = "Sentiment", xlab = "Text")
legend("topright", col = 1:3, legend = colnames(s), lty = 1, cex = 0.7)
```

