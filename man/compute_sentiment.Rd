% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sentomeasures.R
\name{compute_sentiment}
\alias{compute_sentiment}
\title{Computation of document-level sentiment across features and lexicons}
\usage{
compute_sentiment(corpuS, lexicons, how = get_hows()$words)
}
\arguments{
\item{corpuS}{a \code{corpuS} object.}

\item{lexicons}{output from a \code{setup_lexicons()} call.}

\item{how}{a single \code{character} vector defining how aggregation within documents will be performed. For currently
available options on how aggregation can occer, access \code{get_hows()$words}.}
}
\value{
A list containing:
\item{corpuS}{the supplied \code{corpuS} object.}
\item{sentiment}{a sentiment scores \code{data.table} with dates and feature--lexicon sentiment scores columns.}
\item{features}{a \code{character} vector of the different features.}
\item{lexicons}{a \code{character} vector of the different lexicons used.}
}
\description{
Given a corpus of texts, computes sentiment per document starting from the bag-of-words approach,
based on the lexicons provided and a preferred aggregation across words per document scheme. Relies partly on the
\pkg{quanteda} package. The scores computed are net sentiment (sum of positive minus sum of negative scores). For a
separate calculation of positive (resp. negative) sentiment, one has to provide distinct positive (resp. negative)
lexicons. This can be done using the \code{do.split} option in the \code{setup_lexicons()}, which automatically splits
any lexicon into positive and negative polarity.
}
