% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sentiment_engines.R
\name{compute_sentiment}
\alias{compute_sentiment}
\title{Compute document-level sentiment across features and lexicons}
\usage{
compute_sentiment(x, lexicons, how = "proportional", nCore = c(2, 1))
}
\arguments{
\item{x}{either a \code{sentocorpus} object created with \code{\link{sento_corpus}}, a \pkg{quanteda}
\code{\link[quanteda]{corpus}} object, or a \code{character} vector. The latter two do not incorporate a
date dimension. In case of a \code{\link[quanteda]{corpus}} object, the \code{numeric} columns from the
\code{\link[quanteda]{docvars}} are considered as features over which sentiment will be computed. In
case of a \code{character} vector, sentiment is only computed across lexicons.}

\item{lexicons}{output from a \code{\link{setup_lexicons}} call.}

\item{how}{a single \code{character} vector defining how aggregation within documents should be performed. For currently
available options on how aggregation can occur, see \code{\link{get_hows}()$words}. The \code{"tf-idf"} option is not
available when the \code{lexicons} input has a \code{"valence"} element.}

\item{nCore}{a two-length positive \code{numeric} vector to define the parallelisation setup for the sentiment calculation.
The first element is passed on to the \code{numThreads} argument of the \code{\link[RcppParallel]{setThreadOptions}}
function, and parallelizes the sentiment computation across texts, but only when valence shifters are involved. The
second element indicates the number of cores to use for a parallel tokenisation of the input corpus. We use the
\code{\%dopar\%} construct from the \pkg{foreach} package for the latter. By default, \code{nCore = c(2, 1)}. A value of
1 implies no parallelisation. Either parallelisation is expected to improve speed of the sentiment computation only for
sufficiently large corpora, say, in the order of having at least 50,000 documents.}
}
\value{
A \code{list} containing:
\item{corpus}{the supplied \code{x} object, transformed into a \code{\link[quanteda]{corpus}} if a \code{character} vector.}
\item{sentiment}{the sentiment scores \code{data.table} with a \code{"word_count"} column and all
lexicon--feature sentiment scores columns.}
\item{features}{a \code{character} vector of the different features.}
\item{lexicons}{a \code{character} vector of the different lexicons used.}
\item{howWithin}{the supplied \code{how} argument.}

The last three elements are only present if \code{x} is a \code{sentocorpus} object. In that case, the
\code{"sentiment"} \code{data.table} also has a \code{"date"} column, meaning it can be used for further
aggregation into sentiment time series with the \code{\link{perform_agg}} function.
}
\description{
Given a corpus of texts, computes (net) sentiment per document using the bag-of-words approach
based on the lexicons provided and a choice of aggregation across words per document.
}
\details{
For a separate calculation of positive (resp. negative) sentiment, one has to provide distinct positive (resp. negative)
lexicons. This can be done using the \code{do.split} option in the \code{\link{setup_lexicons}} function, which splits out
the lexicons into a positive and a negative polarity counterpart. All \code{NA}s are converted to 0, under the assumption
that this is equivalent to no sentiment. Texts are tokenised as unigrams; punctuation, numbers and symbols are removed, but
not stopwords (see the \code{\link[quanteda]{tokens}} function for more details). The number of words for each document is
computed based on that same tokenisation. All tokens are converted to lowercase, in line with what the
\code{\link{setup_lexicons}} function does for the lexicons and valence shifters.
}
\examples{
data("usnews", package = "sentometrics")
data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")

l1 <- list_lexicons[c("LM_en", "HENRY_en")]
l2 <- setup_lexicons(list_lexicons[c("LM_en", "HENRY_en")], list_valence_shifters[["en"]])

# from a sentocorpus object
corpus <- sento_corpus(corpusdf = usnews)
corpusSample <- quanteda::corpus_sample(corpus, size = 200)
sent <- compute_sentiment(corpusSample, l1, how = "proportionalPol")

# from a character vector
sent <- compute_sentiment(usnews[["texts"]][1:200], l1, how = "counts")

\dontrun{
# from a corpus object, parallelized
corpusQ <- quanteda::corpus(usnews, text_field = "texts")
sent <- compute_sentiment(corpusQ, l2, how = "counts", nCore = c(2, 2))}

}
\author{
Samuel Borms
}
