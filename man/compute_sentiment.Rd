% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sentiment_engines.R
\name{compute_sentiment}
\alias{compute_sentiment}
\title{Compute document-level sentiment across features and lexicons}
\usage{
compute_sentiment(x, lexicons, how = "proportional", tokens = NULL,
  nCore = 2)
}
\arguments{
\item{x}{either a \code{sentocorpus} object created with \code{\link{sento_corpus}}, a \pkg{quanteda}
\code{\link[quanteda]{corpus}} object, or a \code{character} vector. The latter two do not incorporate a
date dimension. In case of a \code{\link[quanteda]{corpus}} object, the \code{numeric} columns from the
\code{\link[quanteda]{docvars}} are considered as features over which sentiment will be computed. In
case of a \code{character} vector, sentiment is only computed across lexicons.}

\item{lexicons}{output from a \code{\link{setup_lexicons}} call, i.e., a named \code{list} of lexicons.}

\item{how}{a single \code{character} vector defining how aggregation within documents should be performed. For currently
available options on how aggregation can occur, see \code{\link{get_hows}()$words}.}

\item{tokens}{a \code{list} of tokenized documents, to specify your own tokenisation scheme. Can result from the
\pkg{quanteda}'s \code{\link[quanteda]{tokens}} function, the \pkg{tokenizers} package, or other. Make sure the tokens are
constructed from (the texts from) the input corpus, are unigrams, and preferably set to lowercase, otherwise, results
will be spurious or errors may occur. By default set to \code{NULL}.}

\item{nCore}{a positive \code{numeric} passed on to the \code{numThreads} argument of the
\code{\link[RcppParallel]{setThreadOptions}} function, to parallelize the sentiment computation across texts. A
value of 1 implies no parallelisation. Parallelisation is expected to improve speed of the sentiment computation
only for sufficiently large corpora, say, in the order of having at least 100,000 documents.}
}
\value{
A \code{list} containing:
\item{corpus}{the supplied \code{x} object (except if \code{x} is a \code{character} vector).}
\item{sentiment}{the sentiment scores \code{data.table} with a \code{"word_count"} column and all
lexicon--feature sentiment scores columns.}
\item{features}{a \code{character} vector of the different features.}
\item{lexicons}{a \code{character} vector of the different lexicons used.}
\item{howWithin}{the supplied \code{how} argument.}

The last three elements are only present if \code{x} is a \code{sentocorpus} object. In that case, the
\code{"sentiment"} \code{data.table} also has a \code{"date"} column, meaning it can be used for further
aggregation into sentiment time series with the \code{\link{perform_agg}} function.
}
\description{
Given a corpus of texts, computes (net) sentiment per document using the bag-of-words approach
based on the lexicons provided and a choice of aggregation across words per document. If the \code{lexicons} argument
has no \code{"valence"} element, the sentiment computed corresponds to simple unigram matching with the lexicons. If
valence shifters are included in \code{lexicons}, these have the effect of modifying the polarity of a word detected from
the lexicon if appearing right before such word (examples: not good, very bad or can't defend).
}
\details{
For a separate calculation of positive (resp. negative) sentiment, one has to provide distinct positive (resp. negative)
lexicons. This can be done using the \code{do.split} option in the \code{\link{setup_lexicons}} function, which splits out
the lexicons into a positive and a negative polarity counterpart. All \code{NA}s are converted to 0, under the assumption
that this is equivalent to no sentiment. If \code{tokens = NULL} (as per default), texts are tokenised as unigrams using
the \code{\link[tokenizers]{tokenize_words}} function. Punctuation and numbers are removed, but not stopwords. The number
of words for each document is computed based on that same tokenisation. All tokens are converted to lowercase, in line
with what the \code{\link{setup_lexicons}} function does for the lexicons and valence shifters.
}
\examples{
data("usnews", package = "sentometrics")
data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")

l1 <- list_lexicons[c("LM_en", "HENRY_en")]
l2 <- setup_lexicons(list_lexicons[c("LM_en", "HENRY_en")], list_valence_shifters[["en"]])

# from a sentocorpus object
corpus <- sento_corpus(corpusdf = usnews)
corpusSample <- quanteda::corpus_sample(corpus, size = 200)
sent <- compute_sentiment(corpusSample, l1, how = "proportionalPol")

# from a character vector
sent <- compute_sentiment(usnews[["texts"]][1:200], l1, how = "counts")

\dontrun{
# from a corpus object, parallelized
corpusQ <- quanteda::corpus(usnews, text_field = "texts")
sent <- compute_sentiment(corpusQ, l2, how = "counts", nCore = 2)}

}
\author{
Samuel Borms
}
