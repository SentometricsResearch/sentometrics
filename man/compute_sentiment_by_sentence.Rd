% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gsoc2019.R
\name{compute_sentiment_by_sentence}
\alias{compute_sentiment_by_sentence}
\title{Calculate sentiment on sentence level}
\usage{
compute_sentiment_by_sentence(x, lexicons, how, tokens = NULL,
  nCore = 1)
}
\arguments{
\item{x}{either a \code{sentocorpus} object created with \code{\link{sento_corpus}}, a \pkg{quanteda}
\code{\link[quanteda]{corpus}} object, or a \code{character} vector. The latter two do not incorporate a
date dimension. In case of a \code{\link[quanteda]{corpus}} object, the \code{numeric} columns from the
\code{\link[quanteda]{docvars}} are considered as features over which sentiment will be computed. In
case of a \code{character} vector, sentiment is only computed across lexicons.}

\item{lexicons}{a \code{sentolexicons} object created using \code{\link{sento_lexicons}}.}

\item{how}{a single \code{character} vector defining how aggregation within documents should be performed. For currently
available options on how aggregation can occur, see \code{\link{get_hows}()$words}.}

\item{tokens}{a \code{list} of tokenized documents, to specify your own tokenization scheme. Can result from the
\pkg{quanteda}'s \code{\link[quanteda]{tokens}} function, the \pkg{tokenizers} package, or other. Make sure the tokens are
constructed from (the texts from) the \code{x} argument, are unigrams, and preferably set to lowercase, otherwise, results
may be spurious and errors could occur. By default set to \code{NULL}.}

\item{nCore}{a positive \code{numeric} that will be passed on to the \code{numThreads} argument of the
\code{\link[RcppParallel]{setThreadOptions}} function, to parallelize the sentiment computation across texts. A
value of 1 (default) implies no parallelization. Parallelization is expected to improve speed of the sentiment
computation only for sufficiently large corpora.}
}
\value{
\code{sentiment} object, i.e., a \code{data.table} containing
the sentiment scores \code{data.table} with an \code{"id"}, a\code{sentence_id}, a \code{"date"} and a \code{"word_count"} column,
and all lexicon--feature sentiment scores columns. To use this sentiment object for aggregation into time series with
the \code{\link{aggregate.sentiment}} function, it should first be aggregated to document level with the
\code{aggregate_sentences}
}
\description{
Sentiment computation on sentence level following the methodology defined in the \pkg{sentimentr} package.
}
\details{
The sentiment calculation on sentence level is similar to the cluster-based approach used in
\code{compute_sentiment} with a few major differences: Firstly, each cluster is an entire sentence instead of
specific number of words. Secondly, an additional valence shifter type 'adversative', can be passed to the \code{sento_lexicon}.
Thirdly, the output of the sentiment is on sentence level and not on document level. Nonetheless, it can be aggregated to document
level as well.
}
\section{Calculation}{

If the \code{lexicons} argument has no \code{"valence"} element, the sentiment computed corresponds to simple unigram
matching with the lexicons [\emph{unigrams} approach]. If the valence
table contains a \code{"t"} column, valence shifters are searched for in a cluster centered around a detected polarity
word [\emph{clusters} approach]. The latter approach is similar along the one utilized by the \pkg{sentimentr} package,
where the cluster is by default 5 words before the polarised word and 2 after. If there are commas around the polarized word,
the cluster is limited(extended) to the words after the previous comma and before the next comma. Within a cluster all the amplifiers
 (\code{t = 2}) and deamplifiers (\code{t = 3}) are counted and weighted by 0.8 (\eqn{0.8 * amp} and \eqn{-0.8 * deamp}). In case,
 the number of negators is not even, the amplifiers are counted as deamplifiers.
 Within the cluster, the adversative words (\code{t = 3}) are also counted. If \eqn{(1 + 0.25 * adv)} is greater than 1,
 the result is added to the amplifier result, if it is less, it is subtracted from the deamplifier result. The impact of
 the deamplifier is limited to -1 if it becomes less than -1. The last step is to take the sum of 1, the amplifier and deamplifier
 result. In case, there is an odd number of negators, the result is multiplied with -1.

 See \code{compute_sentiment} for all the within document (within sentence) weighting schemes that can be used.
}

\examples{
data("usnews", package = "sentometrics")
data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")

l1 <- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")])
l2 <- sento_lexicons(list_lexicons[c("LM_en", "HENRY_en")],
                     list_valence_shifters[["en"]][, c("x", "t")])

# from a sentocorpus object, unigrams approach
corpus <- sento_corpus(corpusdf = usnews)
corpusSample <- quanteda::corpus_sample(corpus, size = 200)
sent1 <- compute_sentiment_by_sentence(corpusSample, l1, how = "squareRootCounts")

# from a corpus object, clusters approach
corpusQ <- quanteda::corpus(usnews, text_field = "texts")
corpusQSample <- quanteda::corpus_sample(corpusQ, size = 200)
sent2 <- compute_sentiment_by_sentence(corpusQSample, l2, how = "counts")

}
\author{
Jeroen Van Pelt, Samuel Borms, Andres Algaba
}
