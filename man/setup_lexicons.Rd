% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sentomeasures_main.R
\name{setup_lexicons}
\alias{setup_lexicons}
\title{Set up lexicons (and valence word list) for use in sentiment analysis}
\usage{
setup_lexicons(lexiconsIn, valenceIn = NULL, do.split = FALSE)
}
\arguments{
\item{lexiconsIn}{a named \code{list} of (raw) lexicons, each element as a \code{data.frame} or a \code{data.table} with
respectively a words column and a polarity score column. Alternatively, a subset of the already formatted built-in lexicons
accessible via \code{list_lexicons} can be declared too, as part of the same list input. If only (some of) the package
built-in lexicons want to be used (with \emph{no} valence shifters), one can simply supply \code{list_lexicons[c(...)]} as
an argument to either \code{\link{sento_measures}} or \code{\link{compute_sentiment}}. However, it is strongly recommended
to pass all lexicons (and a valence word list) to this function first, in any case.}

\item{valenceIn}{a single valence word list as a \code{data.table} or a \code{data.frame} with respectively a words column,
and a score column. This argument can be one of the already formatted built-in valence word lists accessible via
\code{list_valence_shifters}. A word that appears in both a lexicon and the valence word list is prioritized as a
valence shifter. If \code{NULL}, no valence word list is part of this function's output, and is thus not applied in the
sentiment analysis.}

\item{do.split}{a \code{logical} that if \code{TRUE} splits every lexicon into a separate positive polarity and negative
polarity lexicon.}
}
\value{
A \code{list} with each lexicon as a separate element according to its name, as a \code{data.table}, and optionally
an element named \code{valence} that comprises the valence words. Every \code{x} column contains the words, every \code{y}
column contains the polarity score.
}
\description{
Structures provided lexicon(s) and optionally valence words. One can for example combine (part of) the
built-in lexicons from \code{data("list_lexicons")} with other lexicons, and add one of the built-in valence word lists
from \code{data("list_valence_shifters")}. This function makes the output coherent, by converting all words to
lowercase and checking for duplicates. All entries consisting of more than one word are discarded, as required for
bag-of-words sentiment analysis.
}
\examples{
data("list_lexicons", package = "sentometrics")
data("list_valence_shifters", package = "sentometrics")

# lexicons straight from built-in word lists
l1 <- list_lexicons[c("LM_en", "HENRY_en")]

# including a self-made lexicon, with and without valence shifters
lexIn <- c(list(myLexicon = data.table(w = c("nice", "boring"), s = c(2, -1))),
           list_lexicons[c("GI_en")])
valIn <- list_valence_shifters[["en"]]
l2 <- setup_lexicons(lexIn)
l3 <- setup_lexicons(lexIn, valIn)
l4 <- setup_lexicons(lexIn, valIn, do.split = TRUE)

\dontrun{
# include lexicons from lexicon package
lexIn2 <- list(hul = lexicon::hash_sentiment_huliu, joc = lexicon::hash_sentiment_jockers)
l5 <- setup_lexicons(c(lexIn, lexIn2), valIn)}

}
\author{
Samuel Borms
}
